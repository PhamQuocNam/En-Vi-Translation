{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pikke\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import  PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from tokenizers import trainers, Tokenizer, pre_tokenizers, models\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('thainq107/iwslt2015-en-vi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 133317\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer= Tokenizer(models.WordLevel(unk_token='<unk>'))\n",
    "vi_tokenizer = Tokenizer(models.WordLevel(unk_token='<unk>'))\n",
    "\n",
    "en_tokenizer.pre_tokenizer= pre_tokenizers.Whitespace()\n",
    "vi_tokenizer.pre_tokenizer= pre_tokenizers.Whitespace()\n",
    "\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=20000, min_frequency = 2, special_tokens= ['<unk>','<pad>','<bos>','<eos>'])\n",
    "\n",
    "en_tokenizer.train_from_iterator(ds['train']['en'], trainer)\n",
    "vi_tokenizer.train_from_iterator(ds['train']['vi'],trainer)\n",
    "\n",
    "en_tokenizer.save('tokenizer_en.json')\n",
    "vi_tokenizer.save('tokenizer_vi.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = PreTrainedTokenizerFast(tokenizer_file='tokenizer_en.json', bos_token='<bos>',eos_token='<eos>', pad_token='<pad>', unk_token='<unk>')\n",
    "vi_tokenizer = PreTrainedTokenizerFast(tokenizer_file='tokenizer_vi.json', bos_token='<bos>',eos_token='<eos>', pad_token='<pad>', unk_token='<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    src_text = examples['en']\n",
    "    tgt_text = ['<bos> '+ text+' <eos>' for text in examples['vi']]\n",
    "    \n",
    "    src_input_ids = en_tokenizer(src_text,truncation=True,max_length=seq_len,padding='max_length',return_tensors='pt')['input_ids']\n",
    "    tgt_input_ids = vi_tokenizer(tgt_text,truncation=True,max_length=seq_len,padding='max_length',return_tensors='pt')['input_ids']\n",
    "    \n",
    "    return {\n",
    "        'input_ids': src_input_ids,\n",
    "        'labels': tgt_input_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 133317/133317 [00:25<00:00, 5269.89 examples/s]\n",
      "Map: 100%|██████████| 1268/1268 [00:00<00:00, 4977.20 examples/s]\n",
      "Map: 100%|██████████| 1268/1268 [00:00<00:00, 6318.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_ds = ds.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = preprocessed_ds['train'][:4000]\n",
    "val_ds =preprocessed_ds['validation']\n",
    "test_ds = preprocessed_ds['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(preprocessed_data):\n",
    "    for i in range(len(preprocessed_data[:10])):\n",
    "        print('*'*100)\n",
    "        print('English: ',preprocessed_data['en'][i])\n",
    "        print('Input_ids: ',preprocessed_data['input_ids'][i])\n",
    "        print('Vietnam: ', preprocessed_data['vi'][i])\n",
    "        print('Input_ids: ',preprocessed_data['labels'][i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "English:  When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy . &quot;\n",
      "Input_ids:  [219, 15, 25, 131, 4, 15, 199, 47, 280, 25, 6, 301, 30, 6, 510, 4, 12, 15, 1040, 71, 2168, 13, 1003, 172, 8, 24, 7, 3192, 664, 0, 5, 8, 24, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Vietnam:  Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài &quot; Chúng ta chẳng có gì phải ghen tị . &quot;\n",
      "Input_ids:  [2, 316, 7, 122, 235, 4, 44, 80, 50, 0, 3945, 6, 280, 152, 173, 92, 70, 42, 97, 10, 7, 208, 793, 301, 26, 28, 23, 76, 15, 497, 9, 53, 49, 3264, 1789, 5, 26, 28, 23, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "****************************************************************************************************\n",
      "English:  And I was very proud .\n",
      "Input_ids:  [22, 15, 25, 67, 1858, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Vietnam:  Tôi đã rất tự hào về đất nước tôi .\n",
      "Input_ids:  [2, 44, 22, 58, 114, 1134, 38, 280, 152, 7, 5, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "****************************************************************************************************\n",
      "English:  In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .\n",
      "Input_ids:  [134, 259, 4, 21, 638, 13, 150, 11, 94, 1540, 6, 450, 11, 7433, 0, 33, 0, 4, 56, 21, 212, 490, 123, 39, 6, 566, 93, 4, 1343, 14, 564, 4, 893, 3446, 4, 1636, 27, 6, 4981, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Vietnam:  Ở trường , chúng tôi dành rất nhiều thời gian để học về cuộc đời của chủ tịch Kim II- Sung , nhưng lại không học nhiều về thế giới bên ngoài , ngoại trừ việc Hoa Kỳ , Hàn Quốc và Nhật Bản là kẻ thù của chúng tôi .\n",
      "Input_ids:  [2, 656, 166, 4, 12, 7, 551, 58, 61, 139, 190, 37, 66, 38, 109, 344, 13, 348, 2563, 2670, 2526, 130, 0, 4, 91, 56, 14, 66, 61, 38, 42, 97, 254, 359, 4, 774, 973, 54, 974, 1026, 4, 2082, 502, 10, 1242, 1252, 6, 660, 1345, 13, 12, 7, 5, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "****************************************************************************************************\n",
      "English:  Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .\n",
      "Input_ids:  [4707, 15, 400, 2449, 39, 6, 566, 93, 4, 15, 199, 15, 87, 587, 47, 548, 128, 16, 1141, 3446, 4, 481, 296, 1067, 568, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Vietnam:  Mặc dù tôi đã từng tự hỏi không biết thế giới bên ngoài kia như thế nào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộc đời ở BắcTriều Tiên , cho tới khi tất cả mọi thứ đột nhiên thay đổi .\n",
      "Input_ids:  [2, 1730, 478, 7, 22, 255, 114, 179, 14, 74, 42, 97, 254, 359, 516, 40, 42, 73, 4, 91, 7, 245, 80, 50, 90, 39, 104, 63, 109, 344, 35, 0, 3945, 4, 29, 119, 43, 115, 63, 108, 79, 768, 167, 161, 153, 5, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "check(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.input_ids = data['input_ids']\n",
    "        self.labels = data['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor( self.input_ids[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MyDataset(train_ds)\n",
    "val_ds = MyDataset(val_ds)\n",
    "test_ds = MyDataset(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size ,embedding_dim, num_layers, n_heads ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = nn.Embedding(seq_len,embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim,nhead=n_heads, dim_feedforward=4092, dropout=0.2, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer,num_layers )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(embedding_dim,512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512,1024),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        sz= inputs.shape[1]\n",
    "        mask = torch.zeros((sz, sz),device=inputs.device).type(torch.bool)\n",
    "        padding_mask = (inputs==1)\n",
    "        \n",
    "        embeddings = self.embedding(inputs) # NxLxC\n",
    "        \n",
    "        positions = torch.arange(seq_len,device=inputs.device).unsqueeze(0)\n",
    "        positional_encoding = self.positional_encoding(positions)\n",
    "        embeddings+= positional_encoding\n",
    "        inputs = self.encoder(embeddings,mask, padding_mask)  # NxLxC\n",
    "        outputs = self.fc(inputs)\n",
    "        return outputs\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, num_layers, n_heads ,embedding_dim=1024):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = nn.Embedding(seq_len, embedding_dim)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model= embedding_dim, dim_feedforward=2048,dropout=0.2, batch_first=True,nhead=n_heads )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(embedding_dim,512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512,vocab_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, tgt_inputs, abstract_features ):\n",
    "        sz= tgt_inputs.shape[1]\n",
    "        mask = (torch.triu(torch.ones((sz, sz), device=tgt_inputs.device)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        tgt_padding_mask = (tgt_inputs == 1)\n",
    "        \n",
    "        embeddings= self.embedding(tgt_inputs)\n",
    "        positions = torch.arange(seq_len,device=tgt_inputs.device).unsqueeze(0)\n",
    "        positional_encoding = self.positional_encoding(positions)\n",
    "        embeddings+= positional_encoding\n",
    "        \n",
    "        outputs= self.decoder(embeddings, abstract_features, tgt_mask= mask , tgt_key_padding_mask  = tgt_padding_mask)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs\n",
    "        \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, en_tokenizer, vi_tokenizer, num_layers, n_heads, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.bos_idx = en_tokenizer.convert_tokens_to_ids('<bos>')\n",
    "        self.encoder = Encoder(en_tokenizer.vocab_size,embedding_dim,num_layers,n_heads)\n",
    "        self.decoder = Decoder(vi_tokenizer.vocab_size,num_layers, n_heads)\n",
    "    \n",
    "    def forward(self, input_ids, labels):\n",
    "        abstract_features = self.encoder(input_ids)\n",
    "        logits= self.decoder(labels, abstract_features)\n",
    "\n",
    "        return logits.permute(0,2,1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Model(en_tokenizer,vi_tokenizer,2,2,128).to(device)\n",
    "epochs = 1\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion= nn.CrossEntropyLoss(ignore_index=1)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, val_loader, criterion, device):\n",
    "    losses = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for (input_ids, labels) in tqdm.tqdm(val_loader, desc='validation'):\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels= labels.to(device)\n",
    "            \n",
    "            preds = model(input_ids,labels)\n",
    "            loss = criterion(preds[:,:,:-1], labels[:,1:])\n",
    "            \n",
    "            losses.append(loss)\n",
    "        \n",
    "        loss= sum(losses)/len(losses)\n",
    "        print(f'Valid loss: {loss}')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader,  optimizer, criterion, device):\n",
    "    for epoch in tqdm.tqdm(range(epochs), desc='Epoch'):\n",
    "        training_losses= []\n",
    "        model.train()\n",
    "        for idx, (input_ids, labels) in enumerate(train_loader):\n",
    "            input_ids= input_ids.to(device)\n",
    "            labels= labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids,labels)\n",
    "            loss = criterion(logits[:,:,:-1], labels[:,1:])\n",
    "            training_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss = sum(training_losses)/len(training_losses)\n",
    "        print(f'EPOCH {epoch+1}\\t Training Loss: {train_loss}')\n",
    "        eval(model, val_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation: 100%|██████████| 20/20 [00:14<00:00,  1.42it/s]\n",
      "Epoch: 100%|██████████| 1/1 [08:17<00:00, 497.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 6.069946765899658\n",
      "EPOCH 1\t Training Loss: 6.998533725738525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fit(model, train_loader, val_loader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, sample):\n",
    "    model.eval()\n",
    "    input_ids = en_tokenizer(sample,padding= 'max_length', truncation= True,max_length= seq_len)\n",
    "    input_ids = torch.tensor(input_ids,device=model.device).unsqueeze(0)\n",
    "    translated_sentence='<bos>'\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        tokenized_sentence = torch.tensor(vi_tokenizer(translated_sentence,truncation=True, padding='max_length', max_length= 100)['input_ids'],device= device).unsqueeze(0)\n",
    "        preds = model(input_ids,tokenized_sentence)\n",
    "        token_idx = preds.argmax(1)[:,i]\n",
    "        word = vi_tokenizer.convert_ids_to_tokens(token_idx)[-1]\n",
    "        translated_sentence += ' '+ word\n",
    "        \n",
    "        if word =='<eos>':\n",
    "            break\n",
    "    print(f'Origin: {sample}')\n",
    "    print(f'Translation: {translated_sentence}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
